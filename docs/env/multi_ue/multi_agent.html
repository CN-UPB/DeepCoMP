<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>deepcomp.env.multi_ue.multi_agent API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>deepcomp.env.multi_ue.multi_agent</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from ray.rllib.env.multi_agent_env import MultiAgentEnv

from deepcomp.env.single_ue.variants import DatarateMobileEnv, NormDrMobileEnv, RelNormEnv, MaxNormEnv


class MultiAgentMobileEnv(RelNormEnv, MultiAgentEnv):
    &#34;&#34;&#34;
    Multi-UE and multi-agent env.
    Inherits the parent env&#39;s (eg, DatarateMobileEnv) constructor, step, visualization
    &amp; overwrites MultiAgentEnv&#39;s reset and step.
    https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical
    &#34;&#34;&#34;
    def __init__(self, env_config):
        # this calls parent env.__ini__() since MultiAgentEnv doesn&#39;t have an __init__
        super().__init__(env_config)
        # inherits attributes, obs and action space from parent env

        # how to aggregate rewards from multiple UEs (sum or min utility)
        self.reward_agg = env_config[&#39;reward&#39;]

    def get_ue_actions(self, action):
        &#34;&#34;&#34;
        Retrieve the action per UE from the RL agent&#39;s action and return in in form of a dict.
        Does not yet apply actions to env.

        :param action: Action that depends on the agent type (single, central, multi)
        :return: Dict that consistently (indep. of agent type) maps UE (object) --&gt; action
        &#34;&#34;&#34;
        # get action for each UE based on ID
        return {ue: action[ue.id] for ue in self.ue_list if ue.id in action}

    def get_obs(self):
        &#34;&#34;&#34;Return next obs: Dict with UE --&gt; obs&#34;&#34;&#34;
        obs = dict()
        for ue in self.ue_list:
            obs[ue.id] = self.get_ue_obs(ue)
        return obs

    def step_reward(self, rewards):
        &#34;&#34;&#34;
        Return rewards as they are but use UE ID as key instead of UE itself.
        The reward key needs to be same as obs key &amp; sortable not just hashable.
        &#34;&#34;&#34;
        # sum_rewards = sum(rewards.values())
        # return {ue.id: sum_rewards for ue in rewards.keys()}
        # return {ue.id: r for ue, r in rewards.items()}

        # variant: add aggregated utility of UEs at the same BS
        new_rewards = dict()
        for ue, r in rewards.items():
            # initialize to own utility in case the UE is not connected to any BS and has no neighbors
            agg_util = r
            # neighbors include the UE itself
            neighbors = ue.ues_at_same_bs()
            if len(neighbors) &gt; 0:
                # aggregate utility of different UEs as configured
                if self.reward_agg == &#39;sum&#39;:
                    agg_util = sum([rewards[neighbor] for neighbor in neighbors])
                elif self.reward_agg == &#39;min&#39;:
                    agg_util = min([rewards[neighbor] for neighbor in neighbors])
                else:
                    raise NotImplementedError(f&#34;Unexpected reward aggregation: {self.reward_agg}&#34;)
            new_rewards[ue.id] = agg_util
            self.log.debug(&#39;Reward&#39;, ue=ue, neighbors=neighbors, own_r=r, agg_util=agg_util)
        return new_rewards

    def done(self):
        &#34;&#34;&#34;Return dict of dones: UE --&gt; done?&#34;&#34;&#34;
        done = super().done()
        dones = {ue.id: done for ue in self.ue_list}
        dones[&#39;__all__&#39;] = done
        return dones

    def info(self):
        &#34;&#34;&#34;Return info for each UE as dict. Required by RLlib to be similar to obs.&#34;&#34;&#34;
        info_dict = super().info()
        return {ue.id: info_dict for ue in self.ue_list}


class SeqMultiAgentMobileEnv(MultiAgentMobileEnv):
    &#34;&#34;&#34;
    Multi-agent env where all agents observe and act sequentially rather than simultaneously within each time step.
    All agents act sequentially within a single time step before they move and time increments.
    &#34;&#34;&#34;
    def __init__(self, env_config):
        super().__init__(env_config)
        # order of UEs to make sequential decisions; for now identical to list order
        self.ue_order = self.ue_list
        self.ue_order_idx = 0
        self.curr_ue = self.ue_order[self.ue_order_idx]

    def get_obs(self):
        &#34;&#34;&#34;Return only obs for current UE, such that only this UE acts&#34;&#34;&#34;
        return {self.curr_ue.id: self.get_ue_obs(self.curr_ue)}

    def step_reward(self, rewards):
        &#34;&#34;&#34;Only reward for current UE. Calc as before&#34;&#34;&#34;
        new_rewards = super().step_reward(rewards)
        return {self.curr_ue.id: new_rewards[self.curr_ue.id]}

    def done(self):
        &#34;&#34;&#34;Set done for current UE. For all when reaching the last UE&#34;&#34;&#34;
        done = super().done()
        dones = {
            self.curr_ue.id: done,
            &#39;__all__&#39;: done,
        }
        return dones

    def info(self):
        &#34;&#34;&#34;Same for info: Only for curr UE. Then increment to next UE since it&#39;s the last operation in the step&#34;&#34;&#34;
        info_dict = super(MultiAgentMobileEnv, self).info()
        return {self.curr_ue.id: info_dict}

    def step(self, action):
        &#34;&#34;&#34;Overwrite step to do sequential steps per agent without moving UEs and incrementing time in each step&#34;&#34;&#34;
        # when reaching the last UE in the order, move time, UEs, etc
        # if self.ue_order_idx &gt;= len(self.ue_order):
        #     self.ue_order_idx = 0
        #     # move UEs, update drs, increment time
        #     self.move_ues()
        #     self.update_ue_drs_rewards(penalties=None, update_only=True)
        #     self.time += 1
        # self.curr_ue = self.ue_order[self.ue_order_idx]

        # same as in normal step
        prev_obs = self.obs
        action_dict = self.get_ue_actions(action)
        penalties = self.apply_ue_actions(action_dict)
        rewards = self.update_ue_drs_rewards(penalties=penalties)

        # increment UE idx to now handle next user; but do not move or increment time
        if self.ue_order_idx + 1 &lt; len(self.ue_order):
            self.ue_order_idx += 1
        else:
            self.ue_order_idx = 0
            # move UEs, update drs, increment time
            self.move_ues()
            self.update_ue_drs_rewards(penalties=None, update_only=True)
            self.time += 1
        self.curr_ue = self.ue_order[self.ue_order_idx]

        self.obs = self.get_obs()
        reward = self.step_reward(rewards)
        done = self.done()
        info = self.info()
        self.log.info(&#34;Step&#34;, time=self.time, prev_obs=prev_obs, action=action, reward=reward, next_obs=self.obs,
                      done=done)
        return self.obs, reward, done, info</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv"><code class="flex name class">
<span>class <span class="ident">MultiAgentMobileEnv</span></span>
<span>(</span><span>env_config)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-UE and multi-agent env.
Inherits the parent env's (eg, DatarateMobileEnv) constructor, step, visualization
&amp; overwrites MultiAgentEnv's reset and step.
<a href="https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical">https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical</a></p>
<p>Create a new environment object with an OpenAI Gym interface. Required fields in the env_config:</p>
<ul>
<li>episode_length: Total number of simulation time steps in one episode</li>
<li>map: Map object representing the playground</li>
<li>bs_list: List of base station objects in the environment</li>
<li>ue_list: List of UE objects in the environment</li>
<li>seed: Seed for the RNG; for reproducibility. May be None.</li>
</ul>
<p>:param env_config: Dict containing all configuration options for the environment. Required by RLlib.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiAgentMobileEnv(RelNormEnv, MultiAgentEnv):
    &#34;&#34;&#34;
    Multi-UE and multi-agent env.
    Inherits the parent env&#39;s (eg, DatarateMobileEnv) constructor, step, visualization
    &amp; overwrites MultiAgentEnv&#39;s reset and step.
    https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical
    &#34;&#34;&#34;
    def __init__(self, env_config):
        # this calls parent env.__ini__() since MultiAgentEnv doesn&#39;t have an __init__
        super().__init__(env_config)
        # inherits attributes, obs and action space from parent env

        # how to aggregate rewards from multiple UEs (sum or min utility)
        self.reward_agg = env_config[&#39;reward&#39;]

    def get_ue_actions(self, action):
        &#34;&#34;&#34;
        Retrieve the action per UE from the RL agent&#39;s action and return in in form of a dict.
        Does not yet apply actions to env.

        :param action: Action that depends on the agent type (single, central, multi)
        :return: Dict that consistently (indep. of agent type) maps UE (object) --&gt; action
        &#34;&#34;&#34;
        # get action for each UE based on ID
        return {ue: action[ue.id] for ue in self.ue_list if ue.id in action}

    def get_obs(self):
        &#34;&#34;&#34;Return next obs: Dict with UE --&gt; obs&#34;&#34;&#34;
        obs = dict()
        for ue in self.ue_list:
            obs[ue.id] = self.get_ue_obs(ue)
        return obs

    def step_reward(self, rewards):
        &#34;&#34;&#34;
        Return rewards as they are but use UE ID as key instead of UE itself.
        The reward key needs to be same as obs key &amp; sortable not just hashable.
        &#34;&#34;&#34;
        # sum_rewards = sum(rewards.values())
        # return {ue.id: sum_rewards for ue in rewards.keys()}
        # return {ue.id: r for ue, r in rewards.items()}

        # variant: add aggregated utility of UEs at the same BS
        new_rewards = dict()
        for ue, r in rewards.items():
            # initialize to own utility in case the UE is not connected to any BS and has no neighbors
            agg_util = r
            # neighbors include the UE itself
            neighbors = ue.ues_at_same_bs()
            if len(neighbors) &gt; 0:
                # aggregate utility of different UEs as configured
                if self.reward_agg == &#39;sum&#39;:
                    agg_util = sum([rewards[neighbor] for neighbor in neighbors])
                elif self.reward_agg == &#39;min&#39;:
                    agg_util = min([rewards[neighbor] for neighbor in neighbors])
                else:
                    raise NotImplementedError(f&#34;Unexpected reward aggregation: {self.reward_agg}&#34;)
            new_rewards[ue.id] = agg_util
            self.log.debug(&#39;Reward&#39;, ue=ue, neighbors=neighbors, own_r=r, agg_util=agg_util)
        return new_rewards

    def done(self):
        &#34;&#34;&#34;Return dict of dones: UE --&gt; done?&#34;&#34;&#34;
        done = super().done()
        dones = {ue.id: done for ue in self.ue_list}
        dones[&#39;__all__&#39;] = done
        return dones

    def info(self):
        &#34;&#34;&#34;Return info for each UE as dict. Required by RLlib to be similar to obs.&#34;&#34;&#34;
        info_dict = super().info()
        return {ue.id: info_dict for ue in self.ue_list}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deepcomp.env.single_ue.variants.RelNormEnv" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.RelNormEnv">RelNormEnv</a></li>
<li><a title="deepcomp.env.single_ue.variants.BinaryMobileEnv" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.BinaryMobileEnv">BinaryMobileEnv</a></li>
<li><a title="deepcomp.env.single_ue.base.MobileEnv" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv">MobileEnv</a></li>
<li>gym.core.Env</li>
<li>ray.rllib.env.multi_agent_env.MultiAgentEnv</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv">SeqMultiAgentMobileEnv</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.done"><code class="name flex">
<span>def <span class="ident">done</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return dict of dones: UE &ndash;&gt; done?</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def done(self):
    &#34;&#34;&#34;Return dict of dones: UE --&gt; done?&#34;&#34;&#34;
    done = super().done()
    dones = {ue.id: done for ue in self.ue_list}
    dones[&#39;__all__&#39;] = done
    return dones</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_obs"><code class="name flex">
<span>def <span class="ident">get_obs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return next obs: Dict with UE &ndash;&gt; obs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_obs(self):
    &#34;&#34;&#34;Return next obs: Dict with UE --&gt; obs&#34;&#34;&#34;
    obs = dict()
    for ue in self.ue_list:
        obs[ue.id] = self.get_ue_obs(ue)
    return obs</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_actions"><code class="name flex">
<span>def <span class="ident">get_ue_actions</span></span>(<span>self, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve the action per UE from the RL agent's action and return in in form of a dict.
Does not yet apply actions to env.</p>
<p>:param action: Action that depends on the agent type (single, central, multi)
:return: Dict that consistently (indep. of agent type) maps UE (object) &ndash;&gt; action</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ue_actions(self, action):
    &#34;&#34;&#34;
    Retrieve the action per UE from the RL agent&#39;s action and return in in form of a dict.
    Does not yet apply actions to env.

    :param action: Action that depends on the agent type (single, central, multi)
    :return: Dict that consistently (indep. of agent type) maps UE (object) --&gt; action
    &#34;&#34;&#34;
    # get action for each UE based on ID
    return {ue: action[ue.id] for ue in self.ue_list if ue.id in action}</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.info"><code class="name flex">
<span>def <span class="ident">info</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return info for each UE as dict. Required by RLlib to be similar to obs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def info(self):
    &#34;&#34;&#34;Return info for each UE as dict. Required by RLlib to be similar to obs.&#34;&#34;&#34;
    info_dict = super().info()
    return {ue.id: info_dict for ue in self.ue_list}</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.step_reward"><code class="name flex">
<span>def <span class="ident">step_reward</span></span>(<span>self, rewards)</span>
</code></dt>
<dd>
<div class="desc"><p>Return rewards as they are but use UE ID as key instead of UE itself.
The reward key needs to be same as obs key &amp; sortable not just hashable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_reward(self, rewards):
    &#34;&#34;&#34;
    Return rewards as they are but use UE ID as key instead of UE itself.
    The reward key needs to be same as obs key &amp; sortable not just hashable.
    &#34;&#34;&#34;
    # sum_rewards = sum(rewards.values())
    # return {ue.id: sum_rewards for ue in rewards.keys()}
    # return {ue.id: r for ue, r in rewards.items()}

    # variant: add aggregated utility of UEs at the same BS
    new_rewards = dict()
    for ue, r in rewards.items():
        # initialize to own utility in case the UE is not connected to any BS and has no neighbors
        agg_util = r
        # neighbors include the UE itself
        neighbors = ue.ues_at_same_bs()
        if len(neighbors) &gt; 0:
            # aggregate utility of different UEs as configured
            if self.reward_agg == &#39;sum&#39;:
                agg_util = sum([rewards[neighbor] for neighbor in neighbors])
            elif self.reward_agg == &#39;min&#39;:
                agg_util = min([rewards[neighbor] for neighbor in neighbors])
            else:
                raise NotImplementedError(f&#34;Unexpected reward aggregation: {self.reward_agg}&#34;)
        new_rewards[ue.id] = agg_util
        self.log.debug(&#39;Reward&#39;, ue=ue, neighbors=neighbors, own_r=r, agg_util=agg_util)
    return new_rewards</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deepcomp.env.single_ue.variants.RelNormEnv" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.RelNormEnv">RelNormEnv</a></b></code>:
<ul class="hlist">
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.add_new_ue" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.add_new_ue">add_new_ue</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.apply_ue_actions" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.apply_ue_actions">apply_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.calc_reward" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.calc_reward">calc_reward</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.get_max_num_ue" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.get_max_num_ue">get_max_num_ue</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.get_ue_obs" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.BinaryMobileEnv.get_ue_obs">get_ue_obs</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.move_ues" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.move_ues">move_ues</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.render" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.render">render</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.reset" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.reset">reset</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.seed" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.seed">seed</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.set_log_level" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.set_log_level">set_log_level</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.step" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.step">step</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.test_ue_actions" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.test_ue_actions">test_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.single_ue.variants.RelNormEnv.update_ue_drs_rewards" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.update_ue_drs_rewards">update_ue_drs_rewards</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv"><code class="flex name class">
<span>class <span class="ident">SeqMultiAgentMobileEnv</span></span>
<span>(</span><span>env_config)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-agent env where all agents observe and act sequentially rather than simultaneously within each time step.
All agents act sequentially within a single time step before they move and time increments.</p>
<p>Create a new environment object with an OpenAI Gym interface. Required fields in the env_config:</p>
<ul>
<li>episode_length: Total number of simulation time steps in one episode</li>
<li>map: Map object representing the playground</li>
<li>bs_list: List of base station objects in the environment</li>
<li>ue_list: List of UE objects in the environment</li>
<li>seed: Seed for the RNG; for reproducibility. May be None.</li>
</ul>
<p>:param env_config: Dict containing all configuration options for the environment. Required by RLlib.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SeqMultiAgentMobileEnv(MultiAgentMobileEnv):
    &#34;&#34;&#34;
    Multi-agent env where all agents observe and act sequentially rather than simultaneously within each time step.
    All agents act sequentially within a single time step before they move and time increments.
    &#34;&#34;&#34;
    def __init__(self, env_config):
        super().__init__(env_config)
        # order of UEs to make sequential decisions; for now identical to list order
        self.ue_order = self.ue_list
        self.ue_order_idx = 0
        self.curr_ue = self.ue_order[self.ue_order_idx]

    def get_obs(self):
        &#34;&#34;&#34;Return only obs for current UE, such that only this UE acts&#34;&#34;&#34;
        return {self.curr_ue.id: self.get_ue_obs(self.curr_ue)}

    def step_reward(self, rewards):
        &#34;&#34;&#34;Only reward for current UE. Calc as before&#34;&#34;&#34;
        new_rewards = super().step_reward(rewards)
        return {self.curr_ue.id: new_rewards[self.curr_ue.id]}

    def done(self):
        &#34;&#34;&#34;Set done for current UE. For all when reaching the last UE&#34;&#34;&#34;
        done = super().done()
        dones = {
            self.curr_ue.id: done,
            &#39;__all__&#39;: done,
        }
        return dones

    def info(self):
        &#34;&#34;&#34;Same for info: Only for curr UE. Then increment to next UE since it&#39;s the last operation in the step&#34;&#34;&#34;
        info_dict = super(MultiAgentMobileEnv, self).info()
        return {self.curr_ue.id: info_dict}

    def step(self, action):
        &#34;&#34;&#34;Overwrite step to do sequential steps per agent without moving UEs and incrementing time in each step&#34;&#34;&#34;
        # when reaching the last UE in the order, move time, UEs, etc
        # if self.ue_order_idx &gt;= len(self.ue_order):
        #     self.ue_order_idx = 0
        #     # move UEs, update drs, increment time
        #     self.move_ues()
        #     self.update_ue_drs_rewards(penalties=None, update_only=True)
        #     self.time += 1
        # self.curr_ue = self.ue_order[self.ue_order_idx]

        # same as in normal step
        prev_obs = self.obs
        action_dict = self.get_ue_actions(action)
        penalties = self.apply_ue_actions(action_dict)
        rewards = self.update_ue_drs_rewards(penalties=penalties)

        # increment UE idx to now handle next user; but do not move or increment time
        if self.ue_order_idx + 1 &lt; len(self.ue_order):
            self.ue_order_idx += 1
        else:
            self.ue_order_idx = 0
            # move UEs, update drs, increment time
            self.move_ues()
            self.update_ue_drs_rewards(penalties=None, update_only=True)
            self.time += 1
        self.curr_ue = self.ue_order[self.ue_order_idx]

        self.obs = self.get_obs()
        reward = self.step_reward(rewards)
        done = self.done()
        info = self.info()
        self.log.info(&#34;Step&#34;, time=self.time, prev_obs=prev_obs, action=action, reward=reward, next_obs=self.obs,
                      done=done)
        return self.obs, reward, done, info</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv">MultiAgentMobileEnv</a></li>
<li><a title="deepcomp.env.single_ue.variants.RelNormEnv" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.RelNormEnv">RelNormEnv</a></li>
<li><a title="deepcomp.env.single_ue.variants.BinaryMobileEnv" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.BinaryMobileEnv">BinaryMobileEnv</a></li>
<li><a title="deepcomp.env.single_ue.base.MobileEnv" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv">MobileEnv</a></li>
<li>gym.core.Env</li>
<li>ray.rllib.env.multi_agent_env.MultiAgentEnv</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.done"><code class="name flex">
<span>def <span class="ident">done</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Set done for current UE. For all when reaching the last UE</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def done(self):
    &#34;&#34;&#34;Set done for current UE. For all when reaching the last UE&#34;&#34;&#34;
    done = super().done()
    dones = {
        self.curr_ue.id: done,
        &#39;__all__&#39;: done,
    }
    return dones</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.get_obs"><code class="name flex">
<span>def <span class="ident">get_obs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return only obs for current UE, such that only this UE acts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_obs(self):
    &#34;&#34;&#34;Return only obs for current UE, such that only this UE acts&#34;&#34;&#34;
    return {self.curr_ue.id: self.get_ue_obs(self.curr_ue)}</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.info"><code class="name flex">
<span>def <span class="ident">info</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Same for info: Only for curr UE. Then increment to next UE since it's the last operation in the step</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def info(self):
    &#34;&#34;&#34;Same for info: Only for curr UE. Then increment to next UE since it&#39;s the last operation in the step&#34;&#34;&#34;
    info_dict = super(MultiAgentMobileEnv, self).info()
    return {self.curr_ue.id: info_dict}</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Overwrite step to do sequential steps per agent without moving UEs and incrementing time in each step</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, action):
    &#34;&#34;&#34;Overwrite step to do sequential steps per agent without moving UEs and incrementing time in each step&#34;&#34;&#34;
    # when reaching the last UE in the order, move time, UEs, etc
    # if self.ue_order_idx &gt;= len(self.ue_order):
    #     self.ue_order_idx = 0
    #     # move UEs, update drs, increment time
    #     self.move_ues()
    #     self.update_ue_drs_rewards(penalties=None, update_only=True)
    #     self.time += 1
    # self.curr_ue = self.ue_order[self.ue_order_idx]

    # same as in normal step
    prev_obs = self.obs
    action_dict = self.get_ue_actions(action)
    penalties = self.apply_ue_actions(action_dict)
    rewards = self.update_ue_drs_rewards(penalties=penalties)

    # increment UE idx to now handle next user; but do not move or increment time
    if self.ue_order_idx + 1 &lt; len(self.ue_order):
        self.ue_order_idx += 1
    else:
        self.ue_order_idx = 0
        # move UEs, update drs, increment time
        self.move_ues()
        self.update_ue_drs_rewards(penalties=None, update_only=True)
        self.time += 1
    self.curr_ue = self.ue_order[self.ue_order_idx]

    self.obs = self.get_obs()
    reward = self.step_reward(rewards)
    done = self.done()
    info = self.info()
    self.log.info(&#34;Step&#34;, time=self.time, prev_obs=prev_obs, action=action, reward=reward, next_obs=self.obs,
                  done=done)
    return self.obs, reward, done, info</code></pre>
</details>
</dd>
<dt id="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step_reward"><code class="name flex">
<span>def <span class="ident">step_reward</span></span>(<span>self, rewards)</span>
</code></dt>
<dd>
<div class="desc"><p>Only reward for current UE. Calc as before</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_reward(self, rewards):
    &#34;&#34;&#34;Only reward for current UE. Calc as before&#34;&#34;&#34;
    new_rewards = super().step_reward(rewards)
    return {self.curr_ue.id: new_rewards[self.curr_ue.id]}</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv">MultiAgentMobileEnv</a></b></code>:
<ul class="hlist">
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.add_new_ue" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.add_new_ue">add_new_ue</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.apply_ue_actions" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.apply_ue_actions">apply_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.calc_reward" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.calc_reward">calc_reward</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_max_num_ue" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.get_max_num_ue">get_max_num_ue</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_actions" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_actions">get_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_obs" href="../single_ue/variants.html#deepcomp.env.single_ue.variants.BinaryMobileEnv.get_ue_obs">get_ue_obs</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.move_ues" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.move_ues">move_ues</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.render" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.render">render</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.reset" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.reset">reset</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.seed" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.seed">seed</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.set_log_level" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.set_log_level">set_log_level</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.test_ue_actions" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.test_ue_actions">test_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.update_ue_drs_rewards" href="../single_ue/base.html#deepcomp.env.single_ue.base.MobileEnv.update_ue_drs_rewards">update_ue_drs_rewards</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="deepcomp.env.multi_ue" href="index.html">deepcomp.env.multi_ue</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv">MultiAgentMobileEnv</a></code></h4>
<ul class="">
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.done" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.done">done</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_obs" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_obs">get_obs</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_actions" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.get_ue_actions">get_ue_actions</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.info" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.info">info</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.step_reward" href="#deepcomp.env.multi_ue.multi_agent.MultiAgentMobileEnv.step_reward">step_reward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv">SeqMultiAgentMobileEnv</a></code></h4>
<ul class="">
<li><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.done" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.done">done</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.get_obs" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.get_obs">get_obs</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.info" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.info">info</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step">step</a></code></li>
<li><code><a title="deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step_reward" href="#deepcomp.env.multi_ue.multi_agent.SeqMultiAgentMobileEnv.step_reward">step_reward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>